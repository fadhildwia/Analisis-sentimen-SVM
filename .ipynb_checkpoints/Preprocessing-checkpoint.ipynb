{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stp = stopwords.words('indonesian')\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def lower(text):\n",
    "    # Case Folding\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Emoticon\n",
    "def remove_punctuation(text):\n",
    "    # Happy Emoticons\n",
    "    emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', ':d', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "    # Sad Emoticons\n",
    "    emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    " \n",
    "    # All emoticons (happy + sad)\n",
    "    emoticons = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "    text = ' '.join([word for word in text.split() if word not in emoticons])\n",
    "\n",
    "    text = re.sub(r'@[\\w]*', ' ', text)\n",
    "\n",
    "    text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', ' ', text)\n",
    "\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', ' ', text)\n",
    "\n",
    "    text = re.sub(r'^RT[\\s]+', ' ', text)  \n",
    "\n",
    "    text = text.lower()  \n",
    "\n",
    "    text = re.sub(r'[^\\w\\s]+', ' ', text)\n",
    "\n",
    "    text = re.sub(r'[0-9]+', ' ', text)\n",
    "\n",
    "    text = re.sub(r'_', ' ', text)\n",
    "\n",
    "    text = re.sub(r'\\$\\w*', ' ', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopword Removal\n",
    "def remove_stopwords(text):\n",
    "    text = ' '.join([word for word in text.split() if word not in stp])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "def stem_text(text):\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Testing\n",
    "def preprocess_data(text):\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = stem_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open data file .csv\n",
    "tweets_df = pd.read_csv('../Dataset/Data_mentah.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.drop(['Date','Username','Location'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional # Optional # Optional # Optional # Optional # Optional # Optional # Optional # Optional # Optional # Optional # Optional # Optional \n",
    "tweets_df['remove_punctuation'] = tweets_df['Text'].map(lambda x: remove_punctuation(x))\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional # Optional # Optional # Optional # Optional # Optional # Optional # Optional # Optional # Optional # Optional # Optional # Optional \n",
    "tweets_df['remove_stopwords'] = tweets_df['remove_punctuation'].map(lambda x: remove_stopwords(remove_punctuation(x)))\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all step cleansing \n",
    "tweets_df['Text_bersih'] = tweets_df['Text'].map(lambda x: stem_text(remove_stopwords(remove_punctuation(x))))\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Column\n",
    "tweets_df.drop(tweets_df.columns[[0, 1, 2]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Duplicate\n",
    "tweets_df.drop_duplicates(subset =\"Text_bersih\", keep = 'first', inplace = True)\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data crawling to file .csv\n",
    "tweets_df.to_csv('../Dataset/Bersih.csv', index = False, header = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
