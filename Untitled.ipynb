{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import numpy as py\n",
    "import re\n",
    "import string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Key Token\n",
    "\n",
    "mykeys = open('../API_key/apikey.txt', 'r').read().splitlines()\n",
    "\n",
    "api_key = mykeys[0]\n",
    "api_key_secret = mykeys[1]\n",
    "access_token = mykeys[2]\n",
    "access_token_secret = mykeys[3]\n",
    "auth_hendler = tweepy.OAuthHandler(api_key, api_key_secret)\n",
    "auth_hendler.set_access_token(access_token, access_token_secret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crawling data\n",
    "\n",
    "api = tweepy.API(auth_hendler)\n",
    "\n",
    "search_term = 'biznet'\n",
    "tweet_amount = 1000\n",
    "\n",
    "#between two dates (just 10 day)\n",
    "start_date = \"2020-12-31\"\n",
    "end_date = \"2020-11-30\"\n",
    "\n",
    "#Tweepy Cursor\n",
    "tweets = tweepy.Cursor(api.search, q=search_term, lang='id', since=start_date).items()\n",
    "\n",
    "# Pulling information from tweets iterable \n",
    "tweets_ = [[tweet.created_at,tweet.user.screen_name, tweet.user.location, tweet.text] for tweet in tweets]\n",
    "\n",
    "#Make DataFrame for tweets after crawling\n",
    "tweets_list = pd.DataFrame(data=tweets_, columns=['Date','Username','Location','Text'])\n",
    "\n",
    "# Creation of dataframe from tweets list\n",
    "tweets_df = pd.DataFrame(tweets_list)\n",
    "#tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "\n",
    "api = tweepy.API(auth_hendler)\n",
    "\n",
    "search_term = 'biznet'\n",
    "tweet_amount = 1000\n",
    "\n",
    "#Tweepy Cursor\n",
    "tweets = tweepy.Cursor(api.search, q=search_term, lang='id', since=start_date).items(tweet_amount)\n",
    "\n",
    "# Pulling information from tweets iterable \n",
    "tweets_ = [[tweet.created_at,tweet.user.screen_name, tweet.user.location, tweet.text] for tweet in tweets]\n",
    "\n",
    "#Make DataFrame for tweets after crawling\n",
    "tweets_list = pd.DataFrame(data=tweets_, columns=['Date','Username','Location','Text'])\n",
    "\n",
    "# Creation of dataframe from tweets list\n",
    "tweets_df = pd.DataFrame(tweets_list)\n",
    "tweets_df.to_csv('../Dataset/data.csv', index = False, header = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    # Happy Emoticons\n",
    "    emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', ':d', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "    # Sad Emoticons\n",
    "    emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    " \n",
    "    # All emoticons (happy + sad)\n",
    "    emoticons = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "    text = ' '.join([word for word in text.split() if word not in emoticons])\n",
    "\n",
    "    text = re.sub(r'[^\\w\\s]+', ' ', text)\n",
    "\n",
    "    text = re.sub(r'[0-9]+', ' ', text)\n",
    "\n",
    "    text = re.sub(r'_', ' ', text)\n",
    "\n",
    "    text = re.sub(r'\\$\\w*', ' ', text)\n",
    "\n",
    "    text = re.sub(r'^RT[\\s]+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert data to \n",
    "\n",
    "tweets_df.to_csv('../Dataset/data3.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing \n",
    "import pandas as pd\n",
    "text = pd.read_csv('../Dataset/data_labels_test.csv', encoding='latin-1')\n",
    "#tweets.drop(['Date','Username','Location'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text['Text'] = text['Text'].map(lambda x: remove_punctuation(x))\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cek total dari label positif dan negatif\n",
    "text['Labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melabelkan untuk Star 5 label 1, selain 5 label 0\n",
    "label = []\n",
    "for index, row in text.iterrows():\n",
    "    if row[\"Labels\"] == 'positif':\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "        \n",
    "# menambahkan kolom label dan drop kolom Star, menampilkan review berdasarkan terbawah\n",
    "text[\"Labels\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 5)\n",
      "1    0.5\n",
      "0    0.5\n",
      "Name: Labels, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# menyamakan data label 1 dan 0 menjadi 25000 sample\n",
    "s_1 = text[text['Labels']==1].sample(250,replace=True)\n",
    "s_2 = text[text['Labels']==0].sample(250,replace=True)\n",
    "text = pd.concat([s_1, s_2])\n",
    "\n",
    "# menampilkan total data dan total colom, menampilkan perbandingan dari kolom label\n",
    "print(text.shape)\n",
    "print(text['Labels'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step Cleansing\n",
    "#remove @username\n",
    "def remove_pattern(tweet, pattern):\n",
    "    r = re.findall(pattern, tweet)\n",
    "    for i in r:\n",
    "        tweet = re.sub(i, '', tweet)\n",
    "    return tweet    \n",
    "tweets['remove_user'] = py.vectorize(remove_pattern)(tweets['Text'], \"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove(tweet):\n",
    "    #remove angka\n",
    "    tweet = re.sub('[0-9]+', '', tweet)\n",
    "    \n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    \n",
    "     #menghapus url\n",
    "    tweet = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "tweets['remove_http'] = tweets['remove_user'].apply(lambda x: remove(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv('../Dataset/hasiltest1.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop doplicate \n",
    "tweets.drop_duplicates(subset =\"remove_http\", keep = 'first', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stopwords_indonesia = stopwords.words('indonesian')\n",
    "\n",
    "# Contoh kalimat\n",
    "kalimat = 'Dengan Menggunakan Python dan Library Sastrawi saya dapat melakukan proses Stopword Removal'\n",
    "stop = stopwords_indonesia(tweets)\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file csv\n",
    "\n",
    "tweets = pd.read_csv('../Dataset/test.csv',encoding='latin-1')\n",
    "tweets.drop(['Date','Username','Location'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stopword\n",
    "from nltk.corpus import stopwords \n",
    "stopwords_indonesia = stopwords.words('indonesian')\n",
    " \n",
    "#import sastrawi\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "#tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    " \n",
    "# all emoticons (happy + sad)\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    " \n",
    "def clean_tweets(tweet):\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    #remove coma\n",
    "    tweet = re.sub(r',','',tweet)\n",
    "    \n",
    "    #remove angka\n",
    "    tweet = re.sub('[0-9]+', '', tweet)\n",
    " \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    " \n",
    "    tweets_clean = [] \n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_indonesia and # remove stopwords\n",
    "              word not in emoticons and # remove emoticons\n",
    "                word not in string.punctuation): # remove punctuation\n",
    "            #tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word) # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    " \n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['tweet_clean'] = tweets['Text'].apply(lambda x: clean_tweets(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv('../Dataset/hasiltest.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punct\n",
    "def remove_punct(text):\n",
    "    text  = \" \".join([char for char in text if char not in string.punctuation])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['Tweet'] = tweets['tweet_clean'].apply(lambda x: remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTESTTEST\n",
    "\n",
    "slankword = {'aga' : 'sedikit',\n",
    "            'agak' : 'sedikit',\n",
    "            'ancur' : 'hancur',\n",
    "            'bs' : 'bisa',\n",
    "            'bener' : 'benar',\n",
    "            'bgt' : 'banget',\n",
    "            'bngt' : 'banget',\n",
    "            'cm' : 'cuma',\n",
    "            'dr' : 'dari',\n",
    "            'emg' : 'memang',\n",
    "            'emang' : 'memang',\n",
    "            'engga' : 'tidak',\n",
    "            'ga' : 'tidak',\n",
    "            'gblk' : 'goblok',\n",
    "            'gak' : 'tidak',\n",
    "            'gua' : 'aku',\n",
    "            'gue' : 'aku',\n",
    "            'gmn' : 'gimana',\n",
    "            'jd' : 'jadi',\n",
    "            'jg' : 'juga',\n",
    "            'jgn' : 'jangan',\n",
    "            'kek' : 'seperti',\n",
    "            'kenceng' : 'kencang',\n",
    "            'kyk' : 'seperti',\n",
    "            'knp' : 'kenapa',\n",
    "            'knapa' : 'kenapa',\n",
    "            'km' : 'kamu',\n",
    "            'lg' : 'lagi',\n",
    "            'lgi' : 'lagi',\n",
    "            'lu' : 'kamu',\n",
    "            'lemot' : 'lambat',\n",
    "            'mreka' : 'mereka',\n",
    "            'mo' : 'mau',\n",
    "            'mursidah' : 'murah',\n",
    "            'napa' : 'kenapa',\n",
    "            'nnton' : 'nonton',\n",
    "            'nyobain' : 'coba',\n",
    "            'pake' : 'pakai',\n",
    "            'paraaahhh' : 'parah',\n",
    "            'sad' : 'sedih',\n",
    "            'sdh' : 'sudah',\n",
    "            'sm' : 'sama',\n",
    "            'sampe' : 'sampai',\n",
    "            'skrng' : 'sekarang',\n",
    "            'skali' : 'sekali',\n",
    "            'tu' : 'itu',\n",
    "            'tp' : 'tapi',\n",
    "            'trs' : 'terus',\n",
    "            'thanks' : 'terima kasih',\n",
    "            'uda' : 'sudah',\n",
    "            'udah' : 'sudah',\n",
    "            'udahan' : 'sudah',\n",
    "            'ujan' : 'hujan',\n",
    "            'yg' : 'yang',\n",
    "            'yaa' : 'ya'\n",
    "            }\n",
    "\n",
    "komentar = [] #deklarasi variabel komentar pada list\n",
    "komentar = tweets['Text'].values.tolist() #masukan data kedalam list\n",
    "\n",
    "#import library yang di butuhkan\n",
    "from openpyxl import load_workbook #library untuk menampilkan dokumen\n",
    "import pandas as pd #import pandas \n",
    "from nltk.tokenize import word_tokenize #import library nltk - tokenize\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory #import library sastrawi untuk\n",
    "\n",
    "for index, text in enumerate(komentar):\n",
    "    text = text.lower()\n",
    "    splitlines = word_tokenize(text)\n",
    "    for word in splitlines:\n",
    "        if word in slankword:\n",
    "            text = text.replace(word, str(slankword[word]));\n",
    "            komentar[index] = text\n",
    "\n",
    "tweets['Text'] = pd.DataFrame(komentar)\n",
    "tweets\n",
    "\n",
    "#ENDTESTENDTESTENDTESTENDTESTENDTESTENDTESTENDTESTENDTESTENDTESTENDTESTENDTESTENDTESTENDTESTENDTESTENDTESTENDTESTENDTESTENDTESTENDTESTENDTESTENDTESTENDTESTENDTEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library yang di butuhkan\n",
    "from openpyxl import load_workbook #library untuk menampilkan dokumen\n",
    "import pandas as pd #import pandas \n",
    "from nltk.tokenize import word_tokenize #import library nltk - tokenize\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory #import library sastrawi untuk\n",
    "\n",
    "for index, text in enumerate(komentar):\n",
    "    text = text.lower()\n",
    "    splitlines = word_tokenize(text)\n",
    "    for word in splitlines:\n",
    "        if word in slankword:\n",
    "            text = text.replace(word, str(slankword[word]));\n",
    "            komentar[index] = text\n",
    "komentar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "komentar = pd.DataFrame(komentar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "komentar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.sort_values(\"Tweet\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv('../Dataset/data_labels.csv',encoding='utf8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.drop(tweets.columns[[0,2]], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#menghapus tweet kosong\n",
    "tweets.drop_duplicates(subset =\"Tweet\", keep = 'first', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cek total dari label positif dan negatif\n",
    "tweets['Labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melabelkan untuk Star 5 label 1, selain 5 label 0\n",
    "label = []\n",
    "for index, row in tweets.iterrows():\n",
    "    if row[\"Labels\"] == 'positif':\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "        \n",
    "# menambahkan kolom label dan drop kolom Star, menampilkan review berdasarkan terbawah\n",
    "tweets[\"Labels\"] = label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menyamakan data label 1 dan 0 menjadi 25000 sample\n",
    "s_1 = tweets[tweets['Labels']==0].sample(2,replace=True)\n",
    "s_2 = tweets[tweets['Labels']==1].sample(2,replace=True)\n",
    "tweets = pd.concat([s_1, s_2])\n",
    "\n",
    "# menampilkan total data dan total colom, menampilkan perbandingan dari kolom label\n",
    "print(tweets.shape)\n",
    "print(tweets['Labels'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv('../Dataset/tweetcomplete.csv',encoding='utf8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('../Dataset/dataset_clear.csv')\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membagi data train dan data test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# membagi data test size dengan ukuran 0.1\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets['Text'], tweets['Labels'], \n",
    "                                                    test_size=0.5, stratify=tweets['Labels'], random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementasi TF IDF pada dokumen kita\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "print(X_train)\n",
    "#print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    \n",
    "   \n",
    "\n",
    "   \n",
    "    # count vectorizer\n",
    "    data = vectorizer.transform([data])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model klasifikasi SVM\n",
    "from sklearn import svm\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# klasifikasi svm dengan kernel linear\n",
    "clf = svm.SVC(kernel=\"linear\")\n",
    "\n",
    "#cross_val_score(clf, X_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lakukan prediksi SVM pada data test\n",
    "clf.fit(X_train,y_train)\n",
    "predict = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library evaluation \n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_score\n",
    "print(\"f1 score hasil prediksi adalah: \")\n",
    "print(f1_score(y_test, predict))\n",
    "\n",
    "# accuracy score\n",
    "print(\"accuracy score hasil prediksi adalah: \")\n",
    "print(accuracy_score(y_test, predict))\n",
    "\n",
    "# precision score\n",
    "print(\"precision score hasil prediksi adalah: \")\n",
    "print(precision_score(y_test, predict))\n",
    "\n",
    "# recall score\n",
    "print(\"recall score hasil prediksi adalah: \")\n",
    "print(recall_score(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, predict).ravel()\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contoh kalimat \n",
    "review_positif = \"ganti biznet aja dari pada indihomo\"\n",
    "review_negatif = \"biznet masalah mulu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cek prediksi dari kalimat\n",
    "clf.predict(preprocess_data(review_positif))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cek prediksi dari kalimat\n",
    "clf.predict(preprocess_data(review_negatif))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "from operator import itemgetter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = 'I am a graduate. I want to learn Python. I like learning Python. Python is easy. Python is interesting. Learning increases thinking. Everyone should invest time in learning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = doc.split()\n",
    "total_word_length = len(total_words)\n",
    "print(total_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sentences = tokenize.sent_tokenize(doc)\n",
    "total_sent_len = len(total_sentences)\n",
    "print(total_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_score = {}\n",
    "for each_word in total_words:\n",
    "    each_word = each_word.replace('.','')\n",
    "    if each_word not in stop_words:\n",
    "        if each_word in tf_score:\n",
    "            tf_score[each_word] += 1\n",
    "        else:\n",
    "            tf_score[each_word] = 1\n",
    "\n",
    "# Dividing by total_word_length for each dictionary element\n",
    "tf_score.update((x, y/int(total_word_length)) for x, y in tf_score.items())\n",
    "print(tf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sent(word, sentences): \n",
    "    final = [all([w in x for w in word]) for x in sentences] \n",
    "    sent_len = [sentences[i] for i in range(0, len(final)) if final[i]]\n",
    "    return int(len(sent_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_score = {}\n",
    "for each_word in total_words:\n",
    "    each_word = each_word.replace('.','')\n",
    "    if each_word not in stop_words:\n",
    "        if each_word in idf_score:\n",
    "            idf_score[each_word] = check_sent(each_word, total_sentences)\n",
    "        else:\n",
    "            idf_score[each_word] = 1\n",
    "\n",
    "# Performing a log and divide\n",
    "idf_score.update((x, math.log(int(total_sent_len)/y)) for x, y in idf_score.items())\n",
    "\n",
    "print(idf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_score = {key: tf_score[key] * idf_score.get(key, 0) for key in tf_score.keys()}\n",
    "print(tf_idf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(dict_elem, n):\n",
    "    result = dict(sorted(dict_elem.items(), key = itemgetter(1), reverse = True)[:n]) \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_top_n(tf_idf_score, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=['Until the early 1930s, Turkey followed a neutral foreign policy with the West by developing joint friendship and neutrality agreements. ',\n",
    "'These bilateral agreements aligned with Atatürk’s worldview. By the end of 1925, Turkey had signed fifteen joint agreements with Western states.',\n",
    "'In the early 1930s, changes and developments in world politics required Turkey to make multilateral agreements to improve its security. Atatürk strongly believed that close cooperation between the Balkan states based on the principle of equality would have an important effect on European politics.',\n",
    "'These states had been ruled by the Ottoman Empire for centuries and had proved to be a powerful force. While the origins of the Balkan agreement may date as far back as 1925, the Balkan Pact came into being in the mid-1930s.',\n",
    "'Several important developments in Europe helped the original idea materialize, such as improvements in the Turkish-Greek alliance and the rapprochement between Bulgaria and Yugoslavia. The most important factor in driving Turkish foreign policy from the mid-1930s onwards was the fear of Italy.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = CountVectorizer()\n",
    "word_count=doc.fit_transform(docs)\n",
    "word_count.shape\n",
    "print(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_count.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count)\n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=doc.get_feature_names(),columns=[\"idf_weights\"])\n",
    "df_idf.sort_values(by=['idf_weights']).head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vector=tfidf_transformer.transform(word_count)\n",
    "feature_names = doc.get_feature_names()\n",
    "first_document_vector=tf_idf_vector[1]\n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=['tfidf'])\n",
    "df.sort_values(by=['tfidf'],ascending=False).head(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(docs)\n",
    "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[1]\n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=['tfidf'])\n",
    "df.sort_values(by=['tfidf'],ascending=False).head(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import pandas as pd\n",
    "df = pd.read_csv('../Dataset/dataset_clear.csv')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "df['Text'] = df.document.map(lambda x: ' '.join([stemmer.stem(y) for y in x.decode('utf-8').split(' ')]))\n",
    "df.stemmed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "docs = np.array([\n",
    "    'The sun is shining',\n",
    "    'The weather is sweet',\n",
    "    'The sun is shining and the weather is sweet'\n",
    "])\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                             \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "wiki = pd.read_csv('../Dataset/dataset_clear.csv')\n",
    "wiki.drop(['Labels'], axis=1, inplace=True)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features= 1000000)\n",
    "tfidf = tfidf_vectorizer.fit_transform(wiki['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "data = pd.read_csv('../Dataset/dataset_clear.csv')\n",
    "data.drop(['Labels'], axis=1, inplace=True)\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "wl = nltk.WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "      - Remove Punctuations\n",
    "      - Tokenization\n",
    "      - Remove Stopwords\n",
    "      - stemming/lemmatizing\n",
    "    \"\"\"\n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    tokens = re.split(\"\\W+\", text)\n",
    "    text = [word for word in tokens if word not in stops]\n",
    "    text = [wl.lemmatize(word) for word in text]\n",
    "    return text\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=5):\n",
    "    \"\"\"\n",
    "      get the feature names and tf-idf score of top n items in the doc,                 \n",
    "      in descending order of scores. \n",
    "    \"\"\"\n",
    "\n",
    "    # use only top n items from vector.\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    results= {} \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        results[feature_names[idx]] = round(score, 3)\n",
    "\n",
    "    # return a sorted list of tuples with feature name and tf-idf score as its element(in descending order of tf-idf scores).\n",
    "    return sorted(results.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "count_vect = CountVectorizer(analyzer=clean_text, tokenizer = None, preprocessor = None, stop_words = None, max_features = 5000)                                        \n",
    "freq_term_matrix = count_vect.fit_transform(data['Text'])\n",
    "\n",
    "tfidf = TfidfTransformer(norm=\"l2\")\n",
    "tfidf.fit(freq_term_matrix)  \n",
    "\n",
    "feature_names = count_vect.get_feature_names()\n",
    "\n",
    "# sample document\n",
    "doc = 'watched horrid thing TV. Needless say one movies watch see much worse get.'\n",
    "\n",
    "tf_idf_vector = tfidf.transform(count_vect.transform([doc]))\n",
    "\n",
    "coo_matrix = tf_idf_vector.tocoo()\n",
    "tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "sorted_items = sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "# extract only the top n elements.\n",
    "# Here, n is 10.\n",
    "word_tfidf = extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "\n",
    "print(\"{}  {}\".format(\"features\", \"tfidf\"))  \n",
    "for k in word_tfidf:\n",
    "    print(\"{} - {}\".format(k[0], k[1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('../Dataset/dataset_clear.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "df_x=train_df[\"Text\"]  \n",
    "df_y=train_df[\"Labels\"]  \n",
    "cv = TfidfVectorizer()   \n",
    "df_xcv = cv.fit_transform(df_x)  \n",
    "a=df_xcv.toarray()  \n",
    "cv.get_feature_names() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Text']=[\" \".join(review) for review in df['Text'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = matrix(c(1,2,3,4), nrow=2, ncol=2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Year_Array=[2010,2011,2012,2013,2014]\n",
    "Month_Array=['Jan','Feb','Mar','April','May','June','July','Aug','Sep','Oct','Nov','Dec']\n",
    "segment=[1, 1, 3, 5, 2, 1, 1, 1, 2, 1, 6, 1]\n",
    "p=0\n",
    "for p in range(0, len(Year_Array), 1):\n",
    "    c=0\n",
    "    for i in range(0, len(segment),1):\n",
    "        h = segment[i]\n",
    "        \n",
    "        for j in range(0,int(h) , 1):         \n",
    "            print((Year_Array[p]) ,(Month_Array[c]))\n",
    "            \n",
    "            c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
